#### Capstone-606
Based on the categorization of the deep learning models a special attention is given to the attack detection methods which are build on different kinds of architectures such as autoencoders ,recurrent neural networks.
A total of around 1000000 instances were collected and are currently stored in a CSV (Comma Separated Values) file. Link to the dataset :https://drive.google.com/file/d/1HDeyt92iaXASdQ_CsuZbwpSX7yCJEkv0/view?usp=sharing
## Feature_extraction.ipynb: 
This is the first phase of the project where data preprocessing and feature engineering are being performed. In data preprocessing Pandas DataFrame.fillna() is used to replace Null values in dataframe.Dataset csv file has null values, which are displayed as NaN in Data Frame. Just like pandas dropna() method manage and remove Null values from a data frame, fillna() manages and let the user replace NaN values with some value of their own.The dataset have many inf values as well. The simplest way to handle infinity values would be to first replace them (infs) to NaN.Tranforming the prediction target label(y).These are transformers that are not intended to be used on features, only on supervised learning targets (class Labels). In other words This transformer should be used to encode target values, i.e. y, and not the input X. We Encode target labels with value between 0 and n_classes-1 using below API class sklearn.preprocessing.LabelEncoder.The data is cleaned  and a stacked encoder model with comparatively smallest number of number of learnable parameters  8,923  is trained over just few epoch of 5. The number of parameters and epoch are small in number because after feature selection task, the model is trained over a smaller number of features. This output is fed as an input to the BILSTM model and is trained and results are obtained. 
## With_Pca.ipynb: 
In this pca is used to get the list of the features that have the highest variance. The combined features having highest variance is being chosen and list of top best features have been exracted. Then two models are trained without increasing the data of the minority classes.
## Smote(1).ipynb:
In this classification with increased data of the minority classes are generated using SMOTE.
New synthetic data samples are generated first using SMOTE algorithm to overcome data imbalance problem. A stacked encoder model with large number of number of learnable parameters 62,552 is trained over 40 epoch. The number of parameters and epoch are large in number because after data creation , we are left with almost 4 times the number of data samples. The BILSMT is also trained and the latent feature space of the Autoencoder is so discriminative that the second BILSTM model which is used as classifier converges just over a single epoch.  

